## 35　最適化アルゴリズム

Stanには，Stanプログラムで書かれた確率密度の最頻値を見つけ出す最適化アルゴリズムがあります。この最頻値はパラメータ推定値として使うこともできますし，ベイズ事後分布の近似の基礎として使うこともできます；30章で，この点推定の背景が論じられています。

Stanは三つの異なる最適化アルゴリズムを提供しています。ニュートン法と，それに関連する準ニュートン法を2種，すなわちBFGSとL-BFGSです。これについてはNocedal and Wright(2006)を参照してください。そこでこれらすべてのアルゴリズムを完全に説明，分析しています。L-BFGSアルゴリズムがデフォルトの最適化アルゴリズムです。ニュートン法は三つの手法の中で最も効率が悪いものですが，刻み幅が設定できる利点があります。

### 35.1 一般的な設定

最適化アルゴリズムはいずれもiterativeな計算をし，iteration回数の上限を設定することができます。デフォルトの最大反復回数は2000です。

最適化アルゴリズムはいずれも進捗状況を表示させることができます。途中のiterationを保存するかどうかや，進捗状況を表示するかどうかは設定で選べます。

### 35.2 BFGSとL-BFGSの設定

#### 収束のモニタリング

(L-)BFGSの収束を確認するのは，許容範囲の値によって制御され，それらの値のいずれかが満たされれば，アルゴリズムは解に達したとして終了します。収束の検証は，許容範囲のパラメータをゼロにすることで無効にすることができます。収束の検証は以下の通りです。

##### パラメータの収束

パラメータ$\theta_i$を第i番目の反復とすると，許容範囲を表す`tol_param`が以下のようになれば収束したとみなします。

$$|| \theta_i - \theta_{i-1} || < tol_param $$

##### 密度の収束

パラメータ$\theta_i$についての第i番目の反復で，データ$y$に対する(正規化されていない)対数密度である $\log p(\theta_i | y)$は，許容範囲`tol_obj`が以下のようになれば収束したとみなします。

$$ | \log p(\theta_i|y) - \log p(\theta_{i-1}|y) | < tol_obj $$

対数密度が収束したと見なされるのは，相対的な許容範囲が次のようになる時です。

$$ \frac{| \log p(\theta_i|y) - \log p(\theta_{i-1}|y) | }{max(| \log p(\theta_i|y)||\log p(\theta_{i-1}|y)|,10.) } < tol_rel_obj * \epsilon $$

##### 勾配の収束

勾配が0に収束したと考えられるのは，特定の許容範囲`tol_grad`が以下のようになった時です。

$$ ||g_i|| < tol_grad $$

ここで$\nabla_{\theta}$は$\theta$についての勾配演算子であり，$g_i = \nabla_{\theta} \log p(\theta_i | y)$は第i番目の反復における勾配です。

勾配が0に収束したと考えられるのは，指定された相対許容範囲`tol_rel_grad`が以下のようになった時です。

$$ \frac{g_i^T \hat{H}_i^{-1}g_i}{max(|\log p(\theta_i|y)|,1.0)} < tol_rel_grad * \epsilon $$

ここで$\hat{H}_i$は第i番目の反復におけるヘッシアン行列で，$|u|$は$u$の絶対値(L1正規化)で，$||u||$は$u$のベクトル長(L2正規化)，$\epsilon \approx 2\bar{e} - 16$は機械精度です。

#### 初期ステップサイズ

BFGS型の最適化アルゴリウムにおいて，初期ステップサイズのパラメータ$\alpha$を指定することができます。もし最初の反復に長い時間がかかるようであれば(そして多くの関数評価が求められるのであれば)，初期値$\alpha$はだいたい最初の反復で使われた$\alpha$と同じになっています。デフォルト値はかなり小さく，0.001で，これは多くの問題に対応するように設定されています。しかし目標とする関数や初期値によっては，大きすぎたり小さすぎたりするかもしれません。大きすぎたり小さすぎたりするというのは，ちょうどいい幅を見つけるまでに，最初の反復がとても長い時間を必要とする(例えば，もっと多くの勾配評価が必要である)することを意味します。パラメータは深刻な問題ではありませんが，同じモデルを何回も(微調整したり，違うデータで実行したりで)最適化する場合は，実行時間を節約するためにも$\alpha$の調整ができます。

#### L-BFGSの履歴サイズ

L-BFGSはコマンドラインの引数を与えることで，へシアン行列に近似させるのに用いる履歴サイズを制御することができます。その値はパラメータ空間の次元より小さいものであるべきで，一般に，比較的小さな値(5-10)で十分です。初期値は5になっています。

もしL-BFGSがうまく行かず，BFGSがうまく行くようであれば，履歴サイズを大きくすることを考えてください。履歴サイズを増加させることで，メモリーの使用量も増えますが，これは典型的なStanモデルで問題になることはほとんどありません。

### 35.3 一般的な設定オプション

最適化アルゴリズムに対する一般的な設定オプションは，MCMCのものと同じなので，詳しくは33.3節をみてください。

### 35.4 最適化のためのモデルを書く

#### 制約するパラメータとしないパラメータ

制約付き最適化問題について，例えば，標準偏差のパラメータ$\sigma$を$\sigma >0$のようにするとき，より効率の良い方法は，パラメータ`sigma`に制約をかけないことです。そうすることで，最適化関数はすぐに0近い値になり，$\log \sigma$のスケール上で$-\infty$になることはありません。

ヤコビアンによる調整は，事後最頻値に対する問題にはなりません。これはStanが，最適化のときには組み込まれているヤコビアンによる調整を止めているからです。

制約のあるパラメータの台をつかって，制約のないパラメータにするときに重要なのは，その台のための初期化をする必要があります。例えば，次のようにベクトルの宣言をするとき，

```
vecto[M] sigma;
```

デフォルトのランダムな初期値として，制約されてないスケール上で`Uniform(-2,2)`とすることは，$2^{-M}$の偶然で初期値が台におちることを意味します。

どんな最適化問題が与えられても，制約がある場合とない場合，両方のやり方でプログラムを実行して，どれがより効率的かをみることに価値があります。
