## 14. クラスタリングモデル
データを教師なしでグループにまとめる方法は、総じてクラスタリングと呼ばれます。
本章では、広く用いられている統計的なクラスタリングモデルの内2つ、ソフト$K$-means法と潜在ディリクレ配分法(LDA)に関して、Stanでの実装について説明します。
本章にはさらに、教師ありの場合のクラスタリングの一種とみなせるナイーブベイズ分類法が含まれます。
通常これらのモデルは、クラスタの割り当てに離散パラメータを用いて表現されます。
しかしながら、Stanでは離散パラメータを周辺化消去することで、これらのモデルを混合モデルのように実装することができます(10章も参照してみてください)。

### 14.1. ソフト$K$-Means法
$K$-meansクラスタリングとは、$D$次元のベクトルで表わされるデータをクラスタリングする方法です。
具体的に言うと、分類される項目が$N$個存在し、その各々はベクトル$y_{n} \in \mathbb{R}^{D}$で表わされます。
$K$-means法の「ソフト」版では、クラスタの割り当てが確率的になります。

#### 幾何的なハード$K$-Meansクラスタリング
$K$-meansクラスタリングは以下のアルゴリズムの様に、通常幾何的な観点で記述されます(クラスタ数$K$とデータベクトル$y$は入力として仮定します)。

1. $1:N$の各$n$に対し、ベクトル$y_n$を$1:K$のとあるクラスタにランダムに割り当てます
2. 以下の処理を繰り返します
    (a) $1:K$の各クラスタ$k$に対し、そのクラスタに割り当てられたベクトルを平均してクラスタの重心$\mu_{k}$を計算します
    (b) $1:N$の各$n$に対し、$y_n$から$\mu_{k}$への(ユークリッド)距離が最小となるクラスタ$k$に、$y_n$を再度割り当てます
    (c) どのベクトルもクラスタが変わらなくなったら、そのクラスタ割り当てを返します

このアルゴリズムは、停止することが保証されています。

#### ソフト$K$-Meansクラスタリング
ソフト$K$-meansクラスタリングでは、クラスタの割り当ては複数のクラスタに渡る確率分布に基づき扱われます。
ユークリッド距離と共分散が固定された多変量正規モデルの間の関係から、ソフト$K$-means法は、多変量正規分布の混合モデルとして(Stanのコードも)表現することができます。

この全生成モデルでは、$1:N$の各データ点$n$に、次のような対称で一様な確率でクラスタ$z_n \in 1:K$が割り当てられます。

![$$ z_n \sim \mathsf{Categorical}(\mathbf{1}/K), $$](fig/fig01.png)

ここで$\mathbf{1}$は$K$次元の単位ベクトルですので、$\mathbf{1}/K$は対称な$K$単体となります。
このようにして、このモデルではクラスタが割り当てられると、そこからおのずと各データ点が抽出されてゆくことが仮定されます。
「ソフト」さは、どのクラスタがデータ点を生成するかという不確実性のみから発生します。

データ点自体は、割り当てられたクラスタ$z_n$から決定されるパラメータを持つ、多変量正規分布より生成されます。

![$$ y_n \sim \mathsf{Normal}(\mu_{z[n]},  \Sigma_{z[n]}) $$](fig/fig02.png)

本節での実装例では、共分散行列には全クラスタ$k$に共通して固定された単位行列を仮定します。

![$$ \Sigma_k = \text{diag\_matrix}(\mathbf{1}), $$](fig/fig03.png)

こうすると、比例定数を除き、多変量の対数正規分布を次のように直接実装することができます。

![$$ \mathsf{Normal} \left(y_n \mid \mu_k, \text{diag\_matrix}(\mathbf{1})\right) \propto \exp \left(-\frac{1}{2}\sum_{d=1}^{D} (\mu_{k, d}-y_{n, d})^2 \right). $$](fig/fig04.png)

$K$-means法に対する空間的な観点では、上式のかっこの内側の項が、まさにクラスタ平均$\mu_k$からデータ点$y_n$までのユークリッド距離(を半分にして符号を反転した値)になっていることに注意して下さい。

#### ソフト$K$-Means法のStanでの実装
$K$-meansクラスタリングを実装した、次のStanのプログラムを考えましょう。^[このモデルは<https://github.com/stan-dev/example-models/tree/master/misc/cluster>から入手可能です。]
```
data {
  int<lower=0> N;  // データ点の数
  int<lower=1> D;  // 次元数
  int<lower=1> K;  // クラスタ数
  vector[D] y[N];  // 観測値
}
transformed data {
  real<upper=0> neg_log_K;
  neg_log_K <- -log(K);
}
parameters {
  vector[D] mu[K]; // クラスタの平均
}
transformed parameters {
  real<upper=0> soft_z[N,K]; // 規格化されていないクラスタ割当確率の対数
  for (n in 1:N)
    for (k in 1:K)
      soft_z[n,k] <- neg_log_K
                     - 0.5 * dot_self(mu[k] - y[n]);
}
model {
  // 事前分布
  for (k in 1:K)
    mu[k] ~ normal(0,1);

  // 尤度
  for (n in 1:N)
    increment_log_prob(log_sum_exp(soft_z[n]));
}
```
重心のパラメータに関する事前分布は、独立な標準正規分布としています。
この事前分布は別の分布に変更してもよく、問題全般のスケールと位置に適合するような階層モデルにしてもよいでしょう。

パラメータは`mu`のみであり、`mu[k]`はクラスタ$k$の重心となります。
変換パラメータ`soft_z[n]`には、規格化されていないクラスタ割当確率の対数が含まれます。
ベクトル`soft_z[n]`は、Stanの外部か、もしくはモデルの`generated quantities`ブロックの中で、`softmax`関数を用いて規格化された単体に戻すことができます(34.11節もご参照ください)。

#### ソフト$K$-Means法の一般化
共分散行列が単位行列の多変量正規分布は、ユークリッド距離(すなわち$L_2$距離)に比例する対数確率密度を生成します。
ここで分布を変えると、関連する幾何的な距離も別なものに変わります。
例えば、正規分布を二重指数(ラプラス)分布に変えると、$L_1$距離(すなわちマンハッタン距離もしくはタクシー距離)に基づくクラスタリングモデルが生成されます。

$K$-means法を多変量正規分布の視点でとらえる限り、共分散行列を単位行列からクラスタに共通する別の行列に変更しても、結局その共分散行列の逆行列で変換された空間で定義されるユークリッド距離で動作しているのと同じことになります。

なお空間との大域的な類似性はありませんが、共分散行列がクラスタ毎に異なるソフト$K$-means法も一般的です。
この様な場合には、共分散行列に対して階層的な事前分布が用いられる事があります。

### 14.2. クラスタリングにおけるベイズ推定のむずかしさ
クラスタリングモデルのフルベイズ推定はほぼ実行不可能であり、これはパラメータの識別可能性の欠如と事後分布の極度な多峰性の2つの問題によるものです。
20.2節には、ラベルスイッチングに起因する識別不可能性について追加で議論が行われています。

#### 識別不可能性
クラスタの割り当ては、そもそも識別することができません。これは、クラスタの平均ベクトル`mu`を入れ替えたとしても、尤度の同じモデルが導れるためです。
例えば、`mu`の最初の2つのインデックスと各`soft_z[n]`の最初の2つのインデックスを入れ替えても、尤度は(事後分布も)同じになります。(訳注: かっこの中は原文priorですが、posteriorの誤記と捉えました)

このような識別可能性の欠如は、複数のマルコフ連鎖の間でクラスタのパラメータが比較できないことを意味しています。
ソフト$K$-means法におけるパラメータは一つですが、このパラメータが識別されないため、実際収束をモニタする際に問題が生じます。
単一の連鎖の中であっても、連鎖が長すぎたりデータがきれいに分離されないような場合にインデックスの入れ替えが伴うと、クラスタの識別に失敗する可能性すらあります。

#### 多峰性
クラスタリングモデルのもう一つの問題は、事後分布が非常に多峰的になる点にあります。
ある種の多峰性は識別不可能であり、インデックスの入れ替えを招きます。
しかしながらたとえインデックスの問題がなかったとしても、事後分布は非常に多峰的になります。

多峰性が高い場合、ベイズ推定は失敗します。これは、事後分布のあらゆる峰を適切な比率で訪れる方法がなく、従って事後予測の推定に含まれる積分を評価する方法もなくなるためです。

これら2つの問題の観点からクラスタリングモデルのあてはめに関してよく聞くアドバイスは、初期値をたくさん変えて試してみる事であったり、全体的に最も高い確率をもつサンプルを選択する事であったりします。
なお、期待値最大化法や変分ベイズ法のような最適化に基づく点推定量を用いるのもポピュラーであり、サンプリングに基づくアプローチよりずっと効率的な場合があります。

### 14.3. ナイーブベイズ分類法およびクラスタリング法
ナイーブベイズ法は一種の混合モデルであり、観測される項目のラベルにより、分類もしくはクラスタリング(または両者のミックス)に使用することができます。^[クラスタリングではあらゆる混合モデルにおいて識別不可能性が問題となりますが、分類ではそのような問題はありません。なお、クラスタリングのフルベイズ推定がむずかしいにもかかわらず、研究者に利用され続けているのは、予測のためのモデル化というよりも探索的データ分析としての位置づけによるものです。]

混合多項分布モデルは「ナイーブベイズ」ともよばれます。これはこのモデルが、カテゴリを決める多項分布が独立しているという仮定が明らかに成り立たないような分類問題にも適用されてしまうことが多いためです。

ナイーブベイズによる分類とクラスタリングは、多項分布で表せるような構造(以降ではこれを、多項的な構造と呼びます)をもつあらゆるデータに適用できます。
典型的な例は自然言語テキストの分類とクラスタリングであり、これはこの後例として使用します。

この場合の観測データは一連の$M$本の文書から構成され、その文書は$V$種の異なる語彙から抽出される単語の多重集合(bags of words)からなっています。
文書$m$の単語数は$N_m$であり、単語には$w_{m,1}, \ldots, w_{m,N[m]} \in 1:V$というインデックスが振られています。
文書中の単語のインデックスには順番がありますが、このモデルはその順番を考慮しないため、人間の自然言語のモデルとしては明らかに欠点があります。
なお、トピック(もしくはカテゴリ)の数は$K$に固定されています。

混合多項分布モデルでは、各文書$m \in 1:M$に対して、カテゴリカル分布に従い単一のカテゴリ$z_m \in 1:K$が生成されます。

![$$ z_{m} \sim \mathsf{Categorical}(\theta). $$](fig/fig05.png)

$K$単体のパラメータ$\theta$は、データにおける各カテゴリの出現頻度を表しています。

続いて文書のカテゴリに基づき、各文書の単語が、その文書中の他の単語や他の文書中の単語と条件付き独立になるように生成されます。つまり、文書$m$の単語$n$は次のように生成されます。

![$$ w_{m, n} \sim \mathsf{Categorical}(\phi_{z[m]}). $$](fig/fig06.png)

ここでパラメータ$\phi_{z[m]}$は$V$単体であり、これはカテゴリ$z_m$の文書に関する語彙における各単語の確率を表しています。

パラメータの$\theta$と$\phi$の事前分布には、通常対称ディリクレ分布が用いられます。
なお出現頻度$\theta$は、各カテゴリ$k \in 1:K$が等確率となるように固定される場合もあります。

#### 不ぞろいな(Ragged)配列のコード化
前節におけるナイーブベイズモデルの規定では、単語$w$の表記に不ぞろいな配列を使用しました。
Stanは不ぞろいな配列をサポートしていないため、このモデルのコード化には別のやり方(全単語のリストから定まる語彙種別のインデックスを、各単語に付与する方法)が用いられます。
このデータは次のように構成されており、一列目は出現した単語を全て並べて全文書で通して付けたインデックス`n`、二列目は単語`n`に対する語彙種別のインデックス、三列目は単語`n`が含まれる文書のインデックス、となります。

|                        `n` |    `w[n]`    |  `doc[n]` |
|---------------------------:|:------------:|:---------:|
|                          1 |  $w_{1,1}$   |     1     |
|                          2 |  $w_{1,2}$   |     1     |
|                   $\vdots$ |  $\vdots$    | $\vdots$  |
|                      $N_1$ | $w_{1,N[1]}$ |     1     |
|                  $N_1 + 1$ |  $w_{2,1}$   |     2     |
|                  $N_1 + 2$ |  $w_{2,2}$   |     2     |
|                   $\vdots$ |  $\vdots$    | $\vdots$  |
|                $N_1 + N_2$ | $w_{2,N[2]}$ |     2     |
|            $N_1 + N_2 + 1$ |  $w_{3,1}$   |     3     |
|                   $\vdots$ |  $\vdots$    | $\vdots$  |
| `N`${}=\sum_{m=1}^{M} N_m$ | $w_{M,N[M]}$ |    $M$    |

関連するプログラムの変数は、全文書における総単語数`N`、単語の配列`w`、文書を特定する配列`doc`となります。

#### カテゴリのラベルが付与された訓練データが存在する場合の推定
カテゴリが既知の文書が訓練データとして与えられた場合、ナイーブベイズモデルで単体上のパラメータを推定するStanのコードは、次のように書けます。^[このモデルは<https://github.com/stan-dev/example-models/tree/master/misc/cluster>から入手可能です。]
```
data {
  // 訓練データ
  int<lower=1> K;               // トピック数
  int<lower=1> V;               // 語彙数
  int<lower=0> M;               // 文書数
  int<lower=0> N;               // 総単語数
  int<lower=1,upper=K> z[M];    // 文書mにおけるトピック
  int<lower=1,upper=V> w[N];    // 単語n
  int<lower=1,upper=M> doc[N];  // 単語nに対する文書ID
  // 超パラメータ
  vector<lower=0>[K] alpha;     // トピックの事前分布向け
  vector<lower=0>[V] beta;      // 単語の事前分布向け
}
parameters {
  simplex[K] theta;   // トピックの出現頻度
  simplex[V] phi[K];  // トピックkにおける単語の分布
}
model {
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    z[m] ~ categorical(theta);
  for (n in 1:N)
    w[n] ~ categorical(phi[z[doc[n]]]);
}
```
トピックの識別子$z_m$がデータとして宣言され、カテゴリの割り当てが尤度関数の一部として含まれていることに注意してください。

#### カテゴリのラベルが付与された訓練データが存在しない場合の推定
ナイーブベイズモデルは、多項的な構造のデータを、教師なしのやり方で$K$個(一定)のカテゴリにクラスタリングするためにも使用できます。
データの宣言には前節のモデルと同じ変数が含まれますが、トピックのラベル`z`は除かれます。
`z`は離散的ですので、モデルの計算から積分消去しておく必要があります。
この処理は、ナイーブベイズモデルでも他の混合モデルでも同様です。
パラメータは事前分布を除いて先ほどと同じですが、今度の尤度は、文書あたりで見ると、次のようにカテゴリ(トピック)について周辺化した確率として計算されます。

![$$ \begin{aligned}& \log p \left(w_{m,1}, \ldots, w_{m,N_{m}} \mid \theta, \phi \right) \\& \quad = \log \textstyle \sum_{k=1}^{K} \left(\mathsf{Categorical}(k \mid \theta) \times \prod_{n=1}^{N_{m}} \mathsf{Categorical}(w_{m,n} \mid \phi_{k})\right) \\& \quad= \log \textstyle \sum_{k=1}^{K} \exp \left(\log \mathsf{Categorical}(k \mid \theta) + \sum_{n=1}^{N_{m}} \log \mathsf{Categorical}(w_{m,n} \mid \phi_{k}) \right).\end{aligned} $$](fig/fig07.png)

最終行で`log_sum_exp`関数が用いられているのは数値計算を安定させるためであり、結果は対数スケールで返されることになります。
```
model {
  real gamma[M,K];
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    for (k in 1:K)
      gamma[m,k] <- categorical_log(k,theta);
  for (n in 1:N)
    for (k in 1:K)
      gamma[doc[n],k] <- gamma[doc[n],k]
                         + categorical_log(w[n],phi[k]);
  for (m in 1:M)
    increment_log_prob(log_sum_exp(gamma[m]));
}
```
ローカル変数の`gamma[m,k]`は次の値を表しています。

![$$ \gamma_{m,k} = \log \mathsf{Categorical}(k \mid \theta) + \sum_{n=1}^{N_{m}} \log \mathsf{Categorical}(w_{m,n} \mid \phi_{k}). $$](fig/fig08.png)

$\gamma$が与えられた下で、文書$m$にカテゴリ$k$が割り当てられる事後確率は次のようになります。

![$$ \text{Pr}\left[z_{m}=k \mid w, \alpha, \beta \right] = \exp \left(\gamma_{m,k} - \log \sum_{k=1}^{K} \exp(\gamma_{m,k}) \right). $$](fig/fig09.png)

変数`gamma`を`transformed parameters`ブロックで宣言して定義すると、Stanはそのサンプル値を保存します。
そうすれば、規格化された事後確率も生成量として定義できます。

#### ナイーブベイズのフルベイズ推定
Stanでは、ナイーブベイズモデルの事後予測分布をフルベイズ推定する実装も可能です。この場合、ラベルが付与されたデータと付与されていないデータが組み合わされます。
推定対象には、モデルのパラメータと、ラベルが付与されていないデータのカテゴリに関する事後分布の、両方が含まれることになります。
このモデルは本質的に、未知のカテゴリラベルがMCAR (missing completely at random; どの値が欠測するかが完全にランダムであるような欠測)だと仮定したモデルになっています。
欠測データの補完に関する更なる情報については、 (Gelman et al., 2013; Gelman and Hill, 2007) をご参照ください。
またこのモデルは、ラベルの付与されていないデータがパラメータの推定に寄与するため、半教師あり学習の例にもなっています。

Stanでフルベイズ推定を実行するモデルを規定するためには、ラベルが付与されたデータのモデルとラベルが付与されていないデータのモデルを組み合わせます。
後者の文書の集まりはデータとして宣言されますがカテゴリラベルは付与されず、関連して新しい変数`M2`, `N2`, `w2`, `doc2`が設定されます。
超パラメータのみならずカテゴリ数や単語数は両者で共通となっており、宣言も一度のみです。
同様にパラメータも1種類しか存在しません。
続くモデル部には、事前分布に関する共通する記述、ラベルが付与されたデータに関する記述、およびラベルが付与されていないデータに関する記述、が含まれます。

#### モデル更新を伴わない予測
フルベイズ推定の代替手段の一つに、まずラベルの付与されたデータを用いてモデルを推定し、続いてその結果をラベルの付与されていないデータに適用する(ラベルの付与されていないデータに基いてパラメータの推定値は更新しない)、という方法があります。
ラベルの付与されていない文書に関する`gamma`の定義を`generated quantities`ブロックに移行すると、この様な振る舞いを行う実装が可能になります。
この場合その変数はもはや対数確率には寄与しなくなるので、モデルパラメータの推定においても、もはや同時には影響しないことになります。

### 14.4. 潜在ディリクレ配分法
潜在ディリクレ配分法(Latent Dirichlet Allocation、略してLDA)は、ナイーブベイズを一般化した、混合メンバーシップの多項クラスタリングモデル(Blei et al., 2003)です。
LDAの説明で一般的なトピックと文書という用語を用いれば、まず各文書が複数のトピックを持つ様にモデル化され、その混成比に基づいて、あるトピックから各単語が抽出されることになります。

#### LDAモデル
このモデルの基本型では、まず各文書が、固定の超パラメータに基づいて独立に生成されると仮定されます。
そして文書$m$に対する最初のステップとして、$K$個のトピックに渡るトピック分布である単体$\theta_m$が抽出されます。

![$$ \theta_{m} \sim \mathsf{Dirichlet}(\alpha). $$](fig/fig10.png)

ここで事前分布の超パラメータ$\alpha$は固定されており、これは$K$個の正の値のベクトルとなります。
文書中の単語は、この分布$\theta_m$が与えられた条件の下で、各々独立に生成されることになります。
具体的にはまず最初に、文書に固有のトピック分布に基づき、単語に対するトピック$z_{m,n} \in 1:K$が抽出されます。

![$$ z_{m,n} \sim \mathsf{Categorical}(\theta_{m}). $$](fig/fig11.png)

そして、トピック$z_{m, n}$における単語の分布に従って、最終的に単語$w_{m,n}$が抽出されます。

![$$ w_{m,n} \sim \mathsf{Categorical}(\phi_{z[m,n]}). $$](fig/fig12.png)

なお、トピック$k$における単語の分布$\phi_k$にも、ディリクレ事前分布が設定されます。

![$$ \phi_k \sim \mathsf{Dirichlet}(\beta) $$](fig/fig13.png)

ここで$\beta$は固定であり、$V$個の正の値のベクトルとなります。

#### 離散パラメータの積分消去
Stanでは離散パラメータのサンプリングが(まだ)サポートされていませんが、他の混合モデルの場合と同じように、離散パラメータを積分消去して連続パラメータのみの周辺分布を計算できます。
トピックと単語の周辺事後分布は、次のようになります。

![$$ \begin{aligned}p(\theta, \phi \mid w, \alpha, \beta) & \propto p(\theta \mid \alpha) \times p(\phi \mid \beta) \times p(w \mid \theta, \phi) \\& = \prod_{m=1}^{M} p(\theta_{m} \mid \alpha) \times \prod_{k=1}^{K} p(\phi_k \mid \beta) \times \prod_{m=1}^{M} \prod_{n=1}^{M[n]} p(w_{m,n} \mid \theta_m, \phi).\end{aligned} $$](fig/fig14.png)

上式における最後の積の内側の項は単語の確率であり、トピックの割り当てを積分消去することで次のように定義されます。

![$$ \begin{aligned}p(w_{m,n} \mid \theta_{m}, \phi) & = \sum_{z=1}^{K} p(z, w_{m,n} \mid \theta_m,\phi). \\& = \sum_{z=1}^{K} p(z \mid \theta_{m}) \times p(w_{m,n} \mid \phi_z).\end{aligned} $$](fig/fig15.png)

この分布を先ほどの式に代入してスケールを対数に変換すると、Stanで直接実装が可能な式が得られます。

![$$ \begin{aligned}& \log p(\theta, \phi \mid w, \alpha, \beta) \\& \quad = \textstyle \sum_{m=1}^{M} \log \mathsf{Dirichlet}(\theta_m \mid \alpha) + \textstyle \sum_{k=1}^{K} \log \mathsf{Dirichlet}(\phi_k \mid \beta) \\& \qquad + \textstyle \sum_{m=1}^{M} \textstyle \sum_{n=1}^{N[m]} \log \left(\textstyle \sum_{\mathrm{z}=1}^{K} \mathsf{Categorical}(z \mid \theta_m) \times \mathsf{Categorical}(w_{m,n} \mid \phi_z) \right)\end{aligned} $$](fig/fig16.png)

#### LDAの実装
前節で導出した周辺分布に対し、本節でデータ構造をあてはめて記述すると、StanにおけるLDAのプログラムは次のようになります。
```
data {
  int<lower=2> K;               // トピック数
  int<lower=2> V;               // 語彙数
  int<lower=1> M;               // 文書数
  int<lower=1> N;               // 総単語数
  int<lower=1,upper=V> w[N];    // 単語n
  int<lower=1,upper=M> doc[N];  // 単語nに対する文書ID
  vector<lower=0>[K] alpha;     // トピックの事前分布向け
  vector<lower=0>[V] beta;      // 単語の事前分布向け
}
parameters {
  simplex[K] theta[M];   // 文書mにおけるトピックの分布
  simplex[V] phi[K];     // トピックkにおける単語の分布
}
model {
  for (m in 1:M)
    theta[m] ~ dirichlet(alpha);  // 事前分布
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);     // 事前分布
  for (n in 1:N) {
    real gamma[K];
    for (k in 1:K)
      gamma[k] <- log(theta[doc[n],k]) + log(phi[k,w[n]]);
    increment_log_prob(log_sum_exp(gamma));  // 尤度
  }
}
```
他の混合モデルの場合と同じように、log-sum-of-exponents関数は数値演算を安定化させるために使用されています。

#### 相関を持つトピックモデル
文書のトピック分布における相関を考慮するために、(Blei and Lafferty, 2007)ではLDAの変種が提案されており、そこでは文書毎のトピックに関する事前分布が、ディリクレ分布から多変量のロジスティック正規分布に置き換えられています。

この論文の筆者らは、超パラメータが固定の場合を扱っています。
なお論文では共分散の$L_1$正則化推定値が用いられていますが、これは事前分布に二重指数分布を設定した場合の最大事後確率推定値と等価です。
Stanでは最大事後確率推定は(まだ)サポートされていませんので、多変量のロジスティック正規分布の平均と共分散は、データとして規定する必要があります。

##### 超パラメータが固定の場合の相関を持つトピックモデル
前節におけるStanのモデルは、相関を持つトピックモデルを実装するように修正することができます。このためには、トピックの事前分布に関するデータの宣言において、ディリクレ分布における`alpha`を多変量のロジスティック正規分布における平均と共分散で置き換えます。
```
data {
  ... dataブロックの他の記載は、alphaを除いて前と同じ ...
  vector[K] mu;          // トピック分布の事前分布に関する平均
  cov_matrix[K] Sigma;   // トピック分布の事前分布に関する共分散
}
```
そして、ディリクレ分布から単体となるパラメータ`theta`を抽出する代わりに、まず多変量正規分布からパラメータ `eta`を抽出し、次いで`softmax`を用いてその値を単体に変換します。
```
parameters {
  simplex[V] phi[K];   // トピックkにおける単語の分布
  vector[K] eta[M];    // 文書mにおけるトピックの分布
}
transformed parameters {
  simplex[K] theta[M];
  for (m in 1:M)
    theta[m] <- softmax(eta[m]);
}
model {
  for (m in 1:M)
    eta[m] ~ multi_normal(mu,Sigma);
  ... modelブロックの他の記載は、thetaに関する事前分布を除いて前と同じ ...
}
```

##### 相関を持つトピックモデルのフルベイズ推定
先ほどの平均と共分散に事前分布を加えれば、相関を持つトピックモデルに対するフルベイズ推定がStanでもサポートされることになります。
このためには、トピック分布に関する平均`mu`と共分散`Sigma`の宣言を`data`ブロックから`parameters`ブロックに移行し、それらの事前分布をモデル部で与える必要があります。
共分散行列`Sigma`の事前分布は次のようにコード化すると、比較的効率が良く解釈も容易でしょう。
```
... dataブロックは前と同じ(ただしalphaはない) ...
parameters {
  vector[K] mu;              // トピック分布の事前分布に関する平均
  corr_matrix[K] Omega;      // 相関行列
  vector<lower=0>[K] sigma;  // スケール
  vector[K] eta[M];          // 文書mにおけるトピック分布のロジット
  simplex[V] phi[K];         // トピックkにおける単語の分布
}
transformed parameters {
  ... etaに関する記載は前と同じ ...
  cov_matrix[K] Sigma;       // 共分散行列
  for (m in 1:K)
    Sigma[m,m] <- sigma[m] * sigma[m] * Omega[m,m];
  for (m in 1:(K-1)) {
    for (n in (m+1):K) {
      Sigma[m,n] <- sigma[m] * sigma[n] * Omega[m,n];
      Sigma[n,m] <- Sigma[m,n];
    }
  }
}
model {
  mu ~ normal(0,5);       // ベクトル化に対応、散漫な事前分布
  Omega ~ lkj_corr(2.0);  // 単位相関行列への正則化
  sigma ~ cauchy(0,5);    // 制約により半コーシー分布
  ... 単語のサンプリングに関する記載は前と同じ ...
}
```
形状パラメータが$\alpha > 0$の`LkjCorr`分布は、相関行列(すなわち、対角成分が1の対称な正定値行列)と同じ台を持ちます。
この密度は次のように定義されています。

![$$ \mathsf{LkjCorr}(\Omega \mid \alpha) \propto \det(\Omega)^{\alpha-1} $$](fig/fig17.png)

$\alpha = 2$というスケールでは、この分布は単位相関行列寄りの弱情報事前分布となります。
したがって、多変量のロジスティック正規分布の共分散行列$\Sigma$にこの事前分布を適用すると、対角成分への集中度がいくらか増す一方で、そのスケールは`sigma`の事前分布から決定されるため、これらの複合効果がもたらされることになります。
