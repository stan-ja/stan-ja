# Overview
このドキュメントは統計モデリング言語であるStanのユーザーガイド，リファレンスマニュアルです．導入の章ではStanの全体像について紹介しますが，残りの章ではモデルの実際のプログラミングや，Stanのモデリング言語としての詳細な解説を，コードやデータの型も含めて，実践的な解説を行います．

## 1.1 Stan Home Page

最新のコード，例，マニュアル，バグレポート，機能追加の要望など，Stanに関する情報は下記のリンクにあるStanのホームページから参照できます．

[http://mc-stan.org](http://mc-stan.org)

## 1.2 Stanのインターフェース
Stan Projectでは３つのインターフェースをプロジェクトの一部としてサポートしています．モデリング部分やその使い方に関しては３つのインターフェースで共通していてるので，このマニュアルはその３つに共通するモデリング言語としてのマニュアルとなります．
すべてのインターフェースについて初期化やサンプリング，チューニング方法について共通していて，また事後分布を分析する機能についてもおおまかに共有されています．

提供されているすべてのインターフェースについて，getting-started guideやドキュメントが完全なソースコードと共に提供されています．

### CmdStan
CmdStanはコマンドラインからStanを利用することを可能にします．ある意味でCmdStanはStanのリファレンス実装ともいえます．もともとCmdStanのドキュメントはこのドキュメントの一部でしたが，今では独立したドキュメントとなっています．CmdStanのホームページは下記になります

[http://mc-stan.org/cmdstan.html](http://mc-stan.org/cmdstan.html)

### RStan
RStanはRにおけるStanのインターフェースです．RStanは，R2WinBUGSとR2jagsのモデルのように外側からStanを呼び出しているというよりは，むしろRのメモリに対するインターフェースです．RStanのホームページは下記のとおりです．

[http://mc-stan.org/cmdstan.html](http://mc-stan.org/cmdstan.html)

### PyStan
PyStanはPythonにおけるStanのインターフェースです．RStanと同様に外側のStanを呼び出すというよりは，pythonのメモリレベルのインターフェースです．PyStanのホームページは下記です．

[http://mc-stan.org/pystan.html](http://mc-stan.org/pystan.html)


### MatlabStan
MatlabStanはMatlabにおけるStanへのインターフェースです．RstanやPyStanとは異なり,現状MatlabStanはCmdStanのラッパーです．MatlabStanのホームページは下記のとおりです．

[http://mc-stan.org/matlab-stan.html](http://mc-stan.org/matlab-stan.html)

### Stan.jl
Stan.jlはJuliaにおけるStanのインターフェースです．これもMatlabStanと同様に，CmdStanのラッパーです．Stan.jlのホームページは以下のとおりです．

[http://mc-stan.org/julia-stan.html](http://mc-stan.org/julia-stan.html)

### StataStan
StataStanはStataにおけるStanのインターフェースです．MatlabStan，Stan.jl と同様にこれもCmdStanのラッパーです．StataStanのホームページは下記になります．

[http://mc-stan.org/stata-stan.html](http://mc-stan.org/stata-stan.html)


## 1.3 Stanのプログラム
<<<<<<< HEAD
Stanのプログラムは条件付き確率分布 $p(\theta|x, y)$により定義されます．ここで$\theta$はモデリングしたい未知の値の列(例： モデルの変数, 隠れ変数, 欠損データ, 将来の予測値)で，$y$はモデリングされる既知の変数列，$x$はモデリングされない説明変数の列で定数です（例：サイズ，ハイパーパラメータ）．
=======
Stanのプログラムでは条件付き確率分布 $p(\theta|x, y)$ を通して統計モデルが定義されます．ここで$\theta$はモデリングしたい一連の未知の変数(例： モデルの変数, 隠れ変数, 欠測データ, 将来の予測値)で，$y$はモデリングされる既知の変数列，$x$はモデリングされない一連の説明変数と定数です（例：サイズ，ハイパーパラメータ）．
Stanのプログラムは，変数の型宣言と文からなります．変数の型には整数，実数，ベクトル，行列はもちろん，その他の型の（多次元な）配列があり，それぞれ値を制約することもできます．
>>>>>>> 1e09c79b097a4fb7a29d671d2a675bcc12b02ea5

変数は，その役割に応じて，`data`, `transformed data`, `parameters`, `transformed parameters`, `generated quantities`なるブロックの中で定義されます．また制約のないローカル変数はステートメントブロックで定義することができます．
`transformed data`，`transformed parameters`，`generated quantities`のブロックは，そのブロックで宣言された変数の定義文を含んでいます．

特別な`model`ブロックはモデルの対数尤度を定義する文で構成されています．

またBUGS風のサンプリング記法が背後に有る対数確率をインクリメントするための略記として利用することが出来，その値により対数確率関数が定義されます．
対数確率の変数にはユーザ定義関数と，変数の変換からヤコビアンから直接アクセスすることができます．

### 変数の制約
変数の制約はStanにおいて，とくに`parameters`において重要な要素です．Stanが効率的にサンプリングをするためには，制約を課した変数に関しては，モデルブロックにおいてその台（サポート）を含んでいる必要があります（言い換えると，ゼロでない事後確率をもつ必要があります）．

dataブロックと`transformed data`ブロックにおいて，変数の制約はただ入力や変換のエラーを確認するのみです．`transformed parameters`ブロックにおける制約は，`parameters`ブロックにおける制約と同じように満たされなければなりません．さもなくば完全にランダムウォーク時になってしまうか，もしくは失敗することでしょう．`generated quantities`における制約は，必ず満たされる必要があり，さもなくばサンプリングは失敗することでしょう．なぜなら`generated quantities`ブロックが評価される時点で抽出したサンプルを棄却するにはタイミングが遅すぎるためです．

<<<<<<< HEAD
### 変数の制限
変数の制限はStanにおいて，とくにparametersにおいて重要な要素です．Stanが効率的にサンプリングをするためには，制約を課した変数に関しては，モデルブロックにおいてその台（サポート）を含んでいる必要があります（言い換えると，ゼロでない事後確率をもつ必要があります）．

dataブロックとtransformed dataブロックにおいて，変数の制約はただ入力や変換のエラーを確認するのみです．transformed parametersブロックにおける制約は，パラメータの制約，もしくはランダムウォーク時にもその条件を満たす必要があり，さもなくばサンプリングは失敗します．generated quantitiesにおける制約は，サンプリングが成功させるか，もしくは停止させます．そのブロックが評価される時点でそのドローを棄却するにはタイミングが遅すぎるためです．

=======
>>>>>>> 1e09c79b097a4fb7a29d671d2a675bcc12b02ea5
### 実行順序
Stanの文は命令として解釈されるため，順序が重要です．単一の文は変数に対する値の割り付けで，そうした文の列（と必要に応じて局所変数の定義）によりブロックは構成されます．そしてStanもまたRやBUGSで使われていた有限なfor-eachのループを提供しています．

### 確率的プログラミング言語
Stanは命令的で確率的なプログラミング言語です．DSLの一種で，統計的な推論を用いる特定領域のために開発されました．
「確率的」というのは確率変数を正真正銘の第一級オブジェクトとして扱うので，その意味で「確率的」なプログラミング言語です．
Stanでは，全ての変数を確率変数として扱いますが，その確率変数たちが一部は観測されたものとして，また一部は推定分布を推定したいもの，あるいは事後の予測推論に使用されると考えます．観測された確率変数は`data`ブロックの中で宣言され，観測されていない変数は`parameters`ブロックで宣言されます（`transformed parameters`, `generated quantities` の値も含みます．またそれに依存する局所変数も同様です）
また，観測されていない確率変数たちの周辺分布や結合分布からサンプリングを行い，その平均と分散を推定したり，もしくは下流の階層の事後予測推論にそれらの値をプラグインして使用することもできます．

Stanはまた，「命令的」プログラミング言語でもあります．というのもCやFortranなどと同様(C++や，R，Pythonもしくは Javaなども同じ)に，代入，ループ，条件分岐や局所変数，オブジェクトレベルの関数や，配列のようなデータ構造に基づいているからです．
関数型言語と比較すれば，典型的には関数型言語は高階関数（汎関数）や，目的言語に対するプログラミング言語要素のリフレクションが利用できたり，一方で純粋な関数型言語では全面的に代入を許可していなかったりします．
またオブジェクト指向言語は動的に関数が割り当てられるより一般的なデータ型を取り入れています．

Stan言語は，CやRがそうであるのと同様に，チャーチ＝チューリング完全 [Church (1936); Turing (1936); Hopcroft and Motwani (2006)]です．それはチューリングマシン（もしくはC）で計算可能ないかなるプログラムもStanで実装出来ることを意味しています（もちろんその道は険しいですが）．ちなみにチューリング完全であるために求められるのはただ，ループと条件分岐，そしてループの中でサイズが変更できる配列，それで全てです．

## 1.4 コンパイルとStanプログラムの実行
Stanのコードはまず最初にStanのコンパイラ`stanc`によってC++の言語へと変換され，そしてそのC++はプラットフォーム依存の単独で実行可能な形式に変換されます．またStanはWindows，Mac OS X，Linuxなど様々な形式の実行可能形式を出力することが出来ます．
Stanの実行ファイルをモデルを推定するために実行すると，まず既知の $y$ と $x$ を評価し，その妥当性を評価します．そして（独立ではない）同一分布に従うサンプルの列 $\theta^{(1)},\theta^{(2)},\dots$ を生成し，それらは周辺分布 $p(\theta | y, x)$ に従います．

## 1.5 サンプリング
連続値をとるパラメータに対してStanは Hamiltonian Monte Carlo (HMC) Sampling (Du- ane et al., 1987; Neal, 1994, 2011) というある種のマルコフ連鎖モンテカルロ（MCMC）サンプリング(Metropolis et al., 1953)を用います．Stanは離散値をとるパラメータのサンプリングは提供していません．観測値としては離散値を直接利用することが出来ますが，推定するべきパラメータとしてはモデルの外側で周辺化されている必要があります．10章と12章では，いかにして離散で有界なパラメータをモデルの外に和を取り掃き出すか，そしてその消去がもたらす大幅なサンプリングの効率改善について議論します．


HMCは，対数確率の勾配を使うことで，定常分布への収束とパラメータの探索の効率化を促進しています．推定すべき量のベクトル$\theta$は仮想的な粒子の位置に変換されます．それぞれのiterationにおいてそのランダムな運動量を生成し，（負の）対数確率関数を決めているポテンシャルエネルギー内の粒子の経路をシミュレーションします．
ハミルトンの分解からこのポテンシャルの勾配が運動量の変化を与え，この運動量が位置の変化を与えることが分かります．この時間的に連続な変化は，時間をシミュレーションが容易な離散値とみなすleapfrogアルゴリズムによって近似されます．様々なシミュレーションエラーを治すために，またマルコフ連鎖における詳細釣り合い条件を満たすようにするために，メトロポリス法の棄却ステップは逐次適用されます (Metropolis et al., 1953; Hastings, 1970)．

基本的なユークリッド空間上のHamiltonian Monte Carlo法には，振る舞いに対して大きな影響を与える３つのチューニングするべきパラメータがあります．Stanのサンプラはそれらを手動で設定する方法と，ユーザを介さずに自動で設定する方法の両方を提供しています．

ひとつ目のチューニングパラメータはステップサイズで，ハミルトニアンの一時的な単位（すなわち離散化の単位）において測定されたサイズです．Stanではユーザが指定したステップサイズに設定することも出来ますが，双対平均化法(Nesterov, 2009; Hoffman and Gelman, 2011, 2014)によってwarmupの段階で最適なステップサイズを推定することが出来ます．どちらの場合もあり得るステップサイズの区間からステップサイズを抽出するために，追加のランダム化が適用されます(Neal, 2011)．

２つめのチューニングパラメータはイテレーション間のステップ数で，このステップ数とステップサイズの積により全体のHamiltonian simulationにかかる時間が決定します．Stanはこのステップ数も特定のステップ数を指定することが出来ますが，No-U-Turn (NUTS) sampler (Hoffman and Gelman, 2011, 2014)を使う場合はこのステップ数を自動的に決定する事ができます．


３つ目のチューニングパラメータは，仮想粒子の質量行列です．Stanはこの対角行列となる質量行列や完全な質量行列を推定するために焼き捨てを通じて設定され，将来的にはユーザが指定した質量行列をサポートする予定です．
未知のパラメータの列$\mathbb{theta}$の成分$\theta_k$は巨大な対角行列によって標準化されます．一方完全な質量行列を推定すればることは縮尺に加え回転も考慮する事が出来ますが，それと引き換えに，leapfrogの各ステップの行列演算のためにより多くのメモリを必要とします．


### 収束のモニタリングと有効サンプル数
マルコフ連鎖から得られるサンプルは，その連鎖が定常分布に収束したあとに限り，その周辺分布 $p(\theta| y, x)$ に従います．そのMCMCが収束したかどうかを判定する方法には複数ありますが，不幸にもそのうちのいずれも収束を保証しません．Stanにおいて推薦される方法は，それぞれ異なる初期パラメータを用いてランダムに初期化された複数のマルコフ連鎖を走らせ，最初の焼き捨ての部分を取り除いたのこりのチェーンを半分に分割してそのポテンシャルの逓減に関する統計量 $\hat{R}$ (Gelman and Rubin, 1992)を評価します．
もしその結果十分なサイズのサンプルが得られなければ，その焼き捨てなどその全てを含めてiterationの数を２倍にして再びサンプルを開始します．

$M$ 個の独立なサンプルに基づいて母平均を推定する場合，その推定誤差は $\frac{1}{\sqrt{M}}$ に比例します．またMCMCにおけるサンプルのようにサンプル間に正の相関がある場合に関しては，その推定誤差は $\frac{1}{\sqrt{\text{N_EFF}}}$ に比例します．ここで$\text{N_EFF}$ は有効サンプル数です．したがって通常はその推定や推論のタスクに於いて十分な有効サンプル数（やその推定値）を手計算するのはとても標準的な方法です．

### ベイズ的推論とモンテカルロ法
Stanは完全なベイズ推論をサポートするために開発されました．ベイズ的推論は下記の標準化されていないベイズ則

$$
p(\theta|y, x) \propto p(y|\theta, x)p(\theta, x)
$$
にもとづいています．これはパラメータ $\theta$ とデータ $y$ (それと定数 $x$ )からなる事後分布 $p(\theta | y, x)$ がその尤度 $p(y | \theta, x)$  to
事前分布 $p(\theta|x)$ の積に比例することを表します．

Stanがベイズ推論を行うことは，事後確率関数においてこの比例の関係を記述する事で，このベイズ則は事後確率分布が尤度関数と事前分布の積に比例するということと同値です．

また，完全なベイズ推論は事後分布 $p(\theta|y, x)$ に従うパラメータ $\theta$ が取る値の不確実性を伝搬することです．
このことは
事後分布のサンプル列の推論に基づいた推論が，様々な興味深い量，事後平均や事後区間，その他事象の結果や未観測の予測などに繋がることで成立しています．

## 1.6 最適化
Stanはモデルのための最適化に基づく推論もサポートしています．
与えられた事後分布 $p(\theta|y)$ に対してStanは以下で定義される事後分布の最頻値 $\theta^*$ を見つけることが出来ます．

$$
\theta^* = \text{argmax}_\theta p(\theta|y)
$$

ここで記号 $\text{argmax}_v f(v)$ は 関数 $f(v)$ を最大化する $v$ を選ぶことを意味します．

もし事前分布が一様ならば，事後分布の最頻値を計算することは，パラメータの最尤推定（Maximum likelifood estimate = MLE）に対応します．また，事前分布が一様でなければ，この事後分布の最頻値の計算はしばしばMAP（Maximum a posterior）推定と呼ばれます．最適化においては，変数の制約条件に由来する如何なる変換のヤコビアンも無視されます．
このことは 上側，下側の制約条件を変数宣言時に取り除くたくさんの最適化問題に於いて有効で，無効な解を取り除くための棄却にとって変わります．

### 点推定による推論

$\theta^*$ の推定は点推定と呼ばれています．これは事後分布を分布と言うよりむしろ一つの点として要約することを意味します．
もちろん点推定は推定時の分散を考慮しません．
事後予測確率
<!-- typo??/ -->
$p(\hat{y}| y)$
は与えられたデータ $y$ において事後最頻値を$p(\tilde{y} | \theta^*)$ として与えられますが，これらは予測値に不確実性を考慮しないため，たとえ事前分布を持っていたとしてもベイズ的推論ではありません．もし，事後分布の分散が非常に小さくその平均が最頻値に著しく近ければ，点推定の結果は完全なベイズ推論のものとすごく近いものになります．


## 1.7 変分推論法
Stanはベイズ的推論を近似する変分推論もサポートしています(Jordan et al., 1999; Wainwright and Jordan, 2008)．
変分推論により，事後分布の平均値と不確実性を，パラメトリックな分布を真の事後分布に最適にフィットさせた，近似的な事後分布を用いて推論する事が出来ます．
変分推論法は，特に機械学習の分野においてベイズ的な計算にすさまじい影響を与えてきました．例えば，変分推論法は従来のサンプリングによる推定に比べて，巨大なデータにスケールし易いという特徴があります．

変分推論は事後分布 $p(\theta | y)$ をシンプルにパラメトリックな分布 $q(\theta | \phi)$ で近似します．このことは真の事後分布との以下で与えられるKullback-Leibler 情報量を最小化することに対応します．

$$
\phi^* = \text{argmin}_\phi \text{KL}[q(\theta|\phi) \Vert p(\theta | y)]
$$

これはベイズ推論の問題を解がwell-defined最適化問題に帰着できることを意味します．更に変分推論では，近似の精度はモデルごとに異なるに城，サンプリングに比べて計算量のオーダーが小さい事が解っています
変分推論法が点推定のテクニックで無いことに注意すると，結果は事後分布を近似する分布ということが出来ます．

Stanは自動微分変分推論（Automatic Differentiation Variational Inference ＝ ADVI）というStanのデータ変換のライブラリや，自動微分に関するツールボックスを効率化するためにデザインされたアルゴリズムが実装されています (Kucukelbir et al., 2015)．
ADVIは変分推論の厳しい条件などの理論に反して，如何なるStanのモデルにおいても動作します．
