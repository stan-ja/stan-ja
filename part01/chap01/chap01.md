## 1. Overview
このドキュメントは、統計モデリング言語であるStanのユーザーガイド兼リファレンスマニュアルです。この導入の章ではStanの全体像について紹介しますが、残りの章では、モデルの実際のプログラミングや、Stanのモデリング言語としての詳細な解説を、コードやデータの型も含めて、実践的な解説を行います。

### 1.1 Stan Home Page

最新のコード、例、マニュアル、バグレポート、機能追加の要望など、Stanに関する情報は下記のリンクにあるStanのホームページから参照できます。

[http://mc-stan.org](http://mc-stan.org)

### 1.2 Stanのインターフェース
Stan Projectでは３つのインターフェースをプロジェクトの一部としてサポートしています。モデリング部分やその使い方に関しては３つのインターフェースで共通していてるので、このマニュアルはその３つに共通するモデリング言語としてのマニュアルとなります。初期化やサンプリング、チューニング方法についてはすべてのインターフェースで共通していて、また事後分布を分析する機能についてもおおむね共通しています。

提供されているすべてのインターフェースについて、getting-started guideやドキュメントが完全なソースコードと共に提供されています。

#### CmdStan
CmdStanは、コマンドラインからStanを利用することを可能にします。ある意味でCmdStanはStanのリファレンス実装ともいえます。もともとCmdStanのドキュメントはこのドキュメントの一部でしたが、今では独立したドキュメントとなっています。CmdStanのホームページは以下になります。

[http://mc-stan.org/cmdstan.html](http://mc-stan.org/cmdstan.html)

#### RStan
RStanは、RにおけるStanのインターフェースです。R2WinBUGSやR2jagsでは、Rから外部のソフトウェアとしてWinBUGSやJAGSを呼び出していました。RStanではそうではなく、むしろRのメモリを通じたインターフェースになっています。RStanのホームページは以下になります。

[http://mc-stan.org/rstan.html](http://mc-stan.org/cmdstan.html)

#### PyStan
PyStanは、PythonにおけるStanのインターフェースです。外側のStanを呼び出すというよりは、RStanと同様にPythonのメモリレベルのインターフェースです。PyStanのホームページは以下になります。

[http://mc-stan.org/pystan.html](http://mc-stan.org/pystan.html)


##### MatlabStan
MatlabStanは、MATLABにおけるStanへのインターフェースです。RstanやPyStanとは異なり、現状MatlabStanはCmdStanのラッパーです。MatlabStanのホームページは以下になります。

[http://mc-stan.org/matlab-stan.html](http://mc-stan.org/matlab-stan.html)

##### Stan.jl
Stan.jlは、JuliaにおけるStanのインターフェースです。これもMatlabStanと同様に、CmdStanのラッパーです。Stan.jlのホームページは以下になります。

[http://mc-stan.org/julia-stan.html](http://mc-stan.org/julia-stan.html)

##### StataStan
StataStanは、StataにおけるStanのインターフェースです。MatlabStanと同様にCmdStanのラッパーです。StataStanのホームページは以下になります。

[http://mc-stan.org/stata-stan.html](http://mc-stan.org/stata-stan.html)

##### MathematicaStan
MathematicaStanは、MathematicaにおけるStanのインターフェイスです。MatlabStanと同様にCmdStanのラッパーです。MathematicaStanのホームページは以下になります。

[http://mc-stan.org/mathematica-stan.html](http://mc-stan.org/mathematica-stan.html)

### 1.3 Stanのプログラム
Stanのプログラムでは条件付き確率分布 $p(\theta \mid y, x)$ を通して統計モデルが定義されます。ここで$\theta$はモデルに組み込まれる、一連の未知の変数(例： モデルのパラメータ、隠れ変数、欠測データ、将来の予測値)です。$y$はモデルに組み込まれる、一連の値が得られている変数です。$x$はモデルに組み込まれない、一連の説明変数と定数です（例：サイズ、ハイパーパラメータ）。

Stanのプログラムは、変数の型宣言と文（ステートメント）からなります。変数の型には整数、実数、ベクトル、行列はもちろん、その他の型の（多次元な）配列があり、それぞれ値を制約することもできます。変数は、その役割に応じて、`data`・`transformed data`・`parameters`・`transformed parameters`・`generated quantities`というブロックの中で宣言されます。また制約のないローカル変数を各ブロックで定義することもできます。

`transformed data`、`transformed parameters`、`generated quantities`ブロックは、そのブロックで宣言された変数を定義する文を含んでいます。特別な`model`ブロックはモデルの対数尤度を定義する文で構成されています。

また`model`ブロックの中では、Stanが内部で持っている対数確率をインクリメントするための略記として、BUGS風のサンプリング記法を利用できます。対数確率の変数には直接アクセスすることもでき、ユーザによる確率関数の定義や変数変換のヤコビアンの調整を可能にします。

#### 変数の制約
変数の制約はStanにおいて、特にパラメータについて重要です。Stanが効率的にサンプリングをするためには、宣言時の制約を満たすパラメータの値が、`model`ブロックにおいて台（サポート）を持つ必要があります（言い換えると、ゼロでない事後密度をもつ必要があります）。

`data`ブロックと`transformed data`ブロックにおける制約は、データの入力および変換のエラーチェックのために使われるだけです。`transformed parameters`ブロックにおける制約は、`parameters`ブロックにおける制約と同じように満たされなければなりません。さもなくばサンプリングが完全にランダムウォークになってしまうか、失敗することでしょう。`generated quantities`における制約は必ず満たされる必要があり、さもなくばサンプリングが完全に停止することでしょう。なぜなら`generated quantities`ブロックが評価される時点で抽出されたサンプルを棄却するのは遅すぎるからです。

#### 実行順序
Stanの文は手続きとして解釈されるため、順序が重要です。文は最低でも変数に対する値の代入を伴います。一連の文（と必要に応じたローカル変数の定義）によりブロックが構成されます。そしてStanもまたRやBUGSで使われている有限なfor-eachのループを提供しています。

#### 確率的プログラミング言語
Stanは手続き型の確率的プログラミング言語です。DSL（domain-specific language、 ドメイン固有言語）の一種です。それは統計的な推論という特定の目的のために開発されたという意味です。

Stanは、確率変数を正真正銘の第一級オブジェクトとして扱うという意味で確率的プログラミング言語です。Stanでは変数を確率変数として扱いますが、あるものは観測されていたり、またあるものは未知で推定の必要がある（もしくは事後の予測推論に使う必要がある）ものです。観測された確率変数は`data`ブロックの中で宣言され、観測されていない変数はパラメータとして宣言されます（`parameters`ブロックで宣言された変数や、それに依存する、`transformed parameters`ブロック内の変数・`generated quantities`ブロック内の変数・ローカル変数も含みます）。また観測されていない確率変数の周辺分布や同時分布からサンプリングすることもできますし、確率変数の平均や分散を推定することもできますし、二次的な事後の予測推論ではそれらの値をプラグインすることもできます。

StanはCやFortranと同様（C++、R、PythonやJavaなどと部分的に同じ）の手続き型プログラミング言語です。これは、代入、ループ、条件分岐、ローカル変数、オブジェクトレベルの関数や配列のようなデータ構造に基づいているという意味です。関数型言語と比較すれば、関数型言語では典型的には高階関数（汎関数）が許可され、しばしばリフレクション（プログラムの中でそのプログラムに含まれる型や変数/メソッドの情報を参照/操作できるようにする仕組み）が許可されています。一方で純粋な関数型言語では代入が全く許可されていません。またオブジェクト指向言語はより一般的なデータ型を取り入れており、動的に関数などが選ばれて実行されます。

Stanの言語は、CやRと同様に、チャーチ＝チューリング完全 [Church (1936); Turing (1936); Hopcroft and Motwani (2006)]です。このことはチューリングマシン（もしくはC）で計算可能ないかなるプログラムもStanで実装できることを意味しています（もちろん、簡単にとは限りません）。ちなみにチューリング完全であるために求められるのは、ループと条件分岐、そしてループの中でサイズが変更できる配列のみです。

### 1.4 コンパイルとStanプログラムの実行
Stanのコードはまず最初にStanのコンパイラ`stanc`によってC++のプログラムへと変換され、そしてそのC++プログラムは、プラットフォーム依存の単独で実行可能な形式に変換されます。Stanは、Windows、Mac OS X、Linuxといった様々なOSの実行可能形式を出力できます。^[RやPythonなどの高レベルなスクリプト言語から使う場合、Stanのプログラムは動的にリンク可能なオブジェクトファイルに変換されます。]
Stanの実行ファイルを実行すると、まず既知の $y$ と $x$ を読み込み、その妥当性を評価します。そして（独立ではない）同一分布に従うサンプルの列 $\theta^{(1)},\theta^{(2)},\dots$ を生成します。これらは各々、周辺分布 $p(\theta  \mid  y, x)$ に従います。

### 1.5 サンプリング
連続値をとるパラメータに対してStanは Hamiltonian Monte Carlo (HMC) Sampling (Duane et al., 1987; Neal, 1994, 2011) というある種のマルコフ連鎖モンテカルロ（MCMC）サンプリング(Metropolis et al., 1953)を用います。Stanは離散値をとるパラメータのサンプリングは提供していません。観測値としては離散値を直接利用することができますが、離散値をとるパラメータはモデルから周辺化消去されている必要があります。13章と15章では、いかにして有限の離散値をとるパラメータを和をとってモデルから消去するか、そしてその消去がもたらすサンプリング効率の大幅な向上について議論します。

HMCは対数確率関数の勾配を使うことで、定常分布への収束とパラメタの探索の効率化を促進しています。推定すべき量のベクトル$\theta$は仮想的な粒子の位置と解釈されます。それぞれのiterationにおいて、ランダムな運動量を生成し、（負の）対数確率関数を決めているポテンシャルエネルギー内の粒子の経路をシミュレーションします。ハミルトンの分解からこのポテンシャルの勾配が運動量の変化を与え、この運動量が位置の変化を与えることが分かります。この時間的に連続な変化は、時間をシミュレーションが容易な離散値とみなすleapfrogアルゴリズムによって近似されます。続けて、様々なシミュレーションエラーを直し、マルコフ連鎖の遷移において詳細釣り合い条件を満たすようにするために、メトロポリス法の棄却ステップが適用されます (Metropolis et al., 1953; Hastings, 1970)。

基本的なユークリッド空間上のHamiltonian Monte Carlo法には、振る舞いに対して大きな影響を与える3つの「チューニングすべき」パラメータがあります。Stanのサンプラーはそれらを手動で設定する方法と、ユーザを介さずに自動で設定する方法の両方を提供しています。

1つ目のチューニングパラメータはステップサイズで、ハミルトニアンが測定される時間的な単位（すなわち、離散化の単位）です。Stanではユーザが指定したステップサイズに設定することもできますが、双対平均化法(Nesterov, 2009; Hoffman and Gelman, 2011, 2014)によってwarmupの段階で最適なステップサイズを推定することができます。どちらの場合もとり得るステップサイズの区間からステップサイズを抽出するために、追加のランダム化が適用されます(Neal, 2011)。

2つ目のチューニングパラメータはiterationごとのステップ数で、このステップ数と先ほどのステップサイズの積によりハミルトニアンに関する全シミュレーション時間が決まります。Stanはこのステップ数も特定のステップ数を指定することができますが、No-U-Turn (NUTS) sampler (Hoffman and Gelman, 2011, 2014)を使うと、このステップ数をサンプリング中に自動的に決定することができます。

3つ目のチューニングパラメータは仮想粒子の質量行列です。Stanはwarmupの間に対角行列となる質量行列や完全な質量行列を推定するように設定できます。また、将来的にはユーザが指定した質量行列をサポートする予定です。対角行列となる質量行列を推定すると、未知パラメータ系列$\theta$の各要素$\theta_k$のスケールは正規化されます。一方、完全な質量行列を推定すると、スケーリングと回転の両方を考慮することができますが、^[これらの推定された質量行列は大域的です。これは、サンプリングされるパラメータ空間の全ての点に同じ質量行列が適用されるという意味です。Riemann-manifold HMCではこれが一般化され、質量行列によって示唆される曲率が位置ごとに変化することが許されます。]行列演算のためにleapfrogの各ステップにおいて、より多くのメモリと計算を要します。

#### 収束のモニタリングと有効サンプルサイズ
マルコフ連鎖から得られるサンプルは、その連鎖が定常分布に収束したあとに、周辺分布 $p(\theta \mid  y, x)$ から抽出されます。MCMCが収束したかどうかを判定する方法は複数ありますが、残念ながら、そうしたテストをパスしたからといって収束が保証されるものではありません。Stanにおいておすすめの方法は、いくつか異なるパラメータの初期値を用いてランダムに初期化した複数のマルコフ連鎖を走らせ、warmup/adaptationの期間のサンプルを捨て、各チェーンの残りのサンプルを半分に分割することで、潜在的なスケールの逓減に関する統計量 $\hat{R}$ (Gelman and Rubin, 1992)を計算することです。もしその結果、十分なサイズの有効サンプルが得られなければ、iterationの数を2倍にして再びやり直します。warmupなど全て含めてやり直しです。^[十分に有効サンプルが得られないことは、warmupの期間の長さが不十分であることがしばしば原因です。この再実行戦略は、最初に正しいiteration数を推測するよりも、高々約50%多いiterationを消費するだけです。]

$M$ 個の独立に抽出されたサンプルに基づいて母平均を推定する場合、その推定誤差は $\frac{1}{\sqrt{M}}$ に比例します。もしMCMCを使って抽出されたサンプルのように典型的にサンプル間に正の相関があれば、その推定誤差は $\frac{1}{\sqrt{\text{N\_EFF}}}$ に比例します。ここで$\text{N\_EFF}$ は有効サンプルサイズです。したがって、目下の推定や推論のタスクに対し十分な大きさになるまで、有効サンプルサイズ（やその推定値）をモニターするのは標準的な習慣です。

#### ベイズ推定とモンテカルロ法
Stanはフルベイズの推定をサポートするために開発されました。ベイズ推定は下記の正規化されていないベイズ則

$$p(\theta \mid y, x) \propto p(y \mid \theta, x)p(\theta, x)$$

に基づいています。これは、データ $y$（と定数$x$）が与えられたもとでのパラメータ $\theta$ の事後分布 $p(\theta  \mid  y, x)$ が尤度 $p(y  \mid  \theta, x)$ と事前分布 $p(\theta, x)$ の積に比例することを表します。

Stanでは、ベイズモデリングは定係数を除いて事後確率関数のコーディングを伴います。ベイズ則から事後確率分布は定係数を除いて尤度関数と事前確率の積と等価です。

フルベイズ推定では、事後分布$p(\theta|y, x)$によるモデル化において、パラメータ$\theta$の値に関する不確実性が含まれ伝搬されていくことになります。このことは、事後分布からの一連のサンプルについて推定を行うことで実現できます。その際には、事後平均や事後確率区間等を関心のある量のためにプラグイン推定値として利用したり、事後分布に基づいて事象の結果や未観測のデータ値を予測したりします。

### 1.6 最適化
Stanは、最適化に基づく推論もサポートしています。与えられた事後分布 $p(\theta \mid y)$ に対してStanは、以下で定義される事後分布の最頻値 $\theta^\ast$ を見つけることができます。

$$\theta^\ast = \text{argmax}_\theta p(\theta \mid y)$$

ここで $\text{argmax}_v f(v)$ という記法は 関数 $f(v)$ を最大化する $v$ の値を選ぶことを意味します。

もし事前分布が一様分布ならば、事後分布の最頻値は、パラメータの最尤推定値（maximum likelifood estimate、MLE）に対応します。また、事前分布が一様分布でなければ、この事後分布の最頻値はしばしばMAP（maximum a posterior）推定値と呼ばれます。

最適化においては、変数の制約に由来するいかなる変換のヤコビアンも無視されます。多くの最適化問題でより効率的に計算するには、変数宣言時における上下限の制約を取り除いて、代わりに台の範囲外となる無効な解を許さないように`model`ブロックで棄却するようにします。

#### 点推定値による推論

推定値$\theta^\ast$ はいわゆる「点推定値」と呼ばれています。これは事後分布を、分布というよりむしろ一つの点として要約することを意味します。もちろん点推定値それ自体は推定のばらつきを考慮しません。事後予測確率$p(\tilde{y} \mid y)$は、データ $y$ が与えられた場合の事後最頻値を用いて、$p(\tilde{y} | \theta^\ast)$ のように作ることができます。しかし、これらは事後確率に不確実性を考慮しないため、たとえ事前分布を持っていたとしてもベイズ推定ではありません。もし、事後分布の分散が小さく、その平均が最頻値に近ければ、点推定の結果はフルベイズの推定結果と非常に近くなります。


### 1.7 変分推論
Stanは、ベイズ推定を近似する変分推論もサポートしています(Jordan et al., 1999; Wainwright and Jordan, 2008)。変分推論では近似した事後分布を用いて、事後分布の平均値と不確実性を推定することができます。近似した事後分布は、パラメトリックな分布が真の事後分布にあてはまるように最適化して求めます。変分推論は、特に機械学習の分野においてベイズ推定の計算にすさまじい影響を与えてきました。典型的には変分推論は従来のサンプリングによる推定に比べて速く、巨大なデータにスケールしやすいという特徴があります(Hoffman et al., 2013)。

変分推論は、事後分布 $p(\theta  \mid  y)$ をシンプルなパラメトリックな分布 $q(\theta  \mid  \phi)$ で近似します。このことは、以下のようにKullback-Leibler情報量を最小化することで真の事後分布を近似することに対応します。

$$\phi^\ast = \text{argmin}_\phi \text{KL}[q(\theta \mid \phi) \Vert p(\theta  \mid  y)]$$

これはベイズ推定の問題を、解がwell-definedな計量をもつ最適化問題に帰着できることを意味します。変分推論は、サンプリングと比べると桁違いに速く収束します。近似の精度はモデルによって異なります。変分推論が点推定のテクニックではないことに注意すると、結果は事後分布を近似した分布だということができます。

Stanには、自動微分変分推論（Automatic Differentiation Variational Inference、ADVI）というアルゴリズムが実装されています。このアルゴリズムはStanの変数変換ライブラリや自動微分に関するツールボックスを活用するようにデザインされています(Kucukelbir et al., 2015)。ADVIは、変分推論のアルゴリズムを導出するのに通常は必要となる数学的処理をすべて回避し、いかなるStanのモデルにおいても動作します。
